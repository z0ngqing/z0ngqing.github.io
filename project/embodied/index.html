<!DOCTYPE html>
<html lang="en-us">
<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="theme" content="hugo-academic">
    <meta name="generator" content="Hugo 0.55.0" />
    <meta name="author" content="Zongqing Lu">
    <meta name="description" content="Associate Professor">

    <link rel="stylesheet" href="https://z0ngqing.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="https://z0ngqing.github.io/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://z0ngqing.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://z0ngqing.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700|Merriweather|Roboto+Mono">
    <link rel="stylesheet" href="https://z0ngqing.github.io/css/hugo-academic.css">
    


    <link rel="shortcut icon" href="https://z0ngqing.github.io/img/favicon.ico" type="image/x-icon">
    <link rel="canonical" href="https://z0ngqing.github.io/project/embodied/">

    <title>Embodied AI | Zongqing&#39;s Homepage</title>

</head>
<body id="top">


<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
    <div class="container">

        
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://z0ngqing.github.io/">Zongqing&#39;s Homepage</a>
        </div>

        
        <div class="collapse navbar-collapse" id="#navbar-collapse-1">

            
            <ul class="nav navbar-nav navbar-right">
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#top">Home</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#news">News</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#publications">Publications</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#projects">Lab</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#teaching">Teaching</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#services">Services</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#contact">Contact</a></li>
                
            </ul>

        </div>
    </div>
</nav>

<div class="container">

    <article class="article article-project" itemscope itemtype="http://schema.org/Article">
        

        <h1 itemprop="name">Embodied AI</h1>
        
        

<div class="article-metadata">

    
    
    
    <span class="article-tags">
        <i class="fa fa-tags"></i>
        
        <a class="article-tag-link" href="https://z0ngqing.github.io/tags/embodied-ai">Embodied AI</a>, 
        
        <a class="article-tag-link" href="https://z0ngqing.github.io/tags/reinforcement-learning">Reinforcement Learning</a>, 
        
        <a class="article-tag-link" href="https://z0ngqing.github.io/tags/foundation-models">Foundation Models</a>
        
    </span>
    
    

    

</div>


        

        <div align="justify" class="article-style" itemprop="articleBody">
            

<p>Embodied AI focuses on creating robots that can perceive, reason, and act in the physical world. It brings together dexterous hand manipulation, humanoid whole-body control, and foundation models that link perception, language, and action. By combining these elements, embodied AI aims to enable robots that can generalize and perform complex real-world tasks with human-like adaptability.</p>

<h3 id="dexhand-manipulation">Dexhand Manipulation</h3>

<p><video autoplay controls width="550" style="float: right;  margin: 10px 0px 0px 20px;" class="article-style"><source src="/img/project/dexhands.mp4" type="video/mp4"></video>
Dexterous hand manipulation focuses on enabling robots to interact with objects with the precision, adaptability, and coordination of human hands. This research area explores how to control high-degree-of-freedom robotic hands to grasp, reorient, and manipulate objects of varying shapes, sizes, and materials in dynamic environments. It involves advances in tactile sensing, motor control, and learning-based methods that allow robots to perform complex in-hand manipulations, such as turning, sliding, or regrasping objects. By combining physical modeling with data-driven approaches, dexterous hand manipulation aims to achieve fine motor skills essential for tasks in manufacturing, service robotics, and daily human environments.</p>

<p><br></p>

<h3 id="humanoid-whole-body-control">Humanoid Whole-Body Control</h3>

<p><video autoplay controls width="550" style="float: right;  margin: 10px 0px 0px 20px;" class="article-style"><source src="/img/project/humanoid.mp4" type="video/mp4"></video>
Humanoid whole-body control focuses on enabling humnanoid robots to move, balance, and interact dynamically with their environments. This research area addresses the challenge of coordinating multiple joints and limbs to achieve stable, agile, and adaptive behaviors such as walking, running, lifting, or manipulating objects while maintaining balance. It integrates principles from control theory, optimization, and machine learning to manage the complex coupling between locomotion and manipulation. By developing controllers that can reason about contact forces, dynamics, and motion planning in real time, humanoid whole-body control aims to create robots capable of performing versatile, coordinated actions in unstructured and human-centered environments.</p>

<p><br></p>

<h3 id="foundation-models">Foundation Models</h3>

<p><video autoplay controls width="550" style="float: right;  margin: 10px 0px 0px 20px;" class="article-style"><source src="/img/project/videos.mov" type="video/mp4"></video>
Foundation models for embodied AI aim to create large, general-purpose models that unify perception, action, and reasoning for robots and embodied agents. A key research focus is leveraging human data—including videos, motion capture, demonstrations, and language annotations—to teach these models how humans interact with the physical world. By learning from large-scale, diverse human behaviors, these models can acquire priors about object manipulation, body coordination, and goal-directed actions, enabling better generalization to new tasks and environments. Integrating human data helps bridge the gap between human intelligence and robotic capability, providing a scalable pathway for training embodied agents that act naturally, safely, and efficiently in real-world settings.</p>

        </div>


        <div align="justify" class="article-style" itemprop="articleBody">
            
                
            
                
            
                
            
                
            
                
            
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
            
                
            
                
            
        </div>

        <div>
        
            <br>
            <h3>Publications</h3>
            
                
                    
                
            
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/unicode/" itemprop="url">[ECCV&#39;24] UniCode: Learning a Unified Codebook for Multimodal Large Language Models</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Sipeng Zheng, Bohan Zhou, Yicheng Feng, Ye Wang, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    European Conference on Computer Vision (ECCV), Sep. 29- Oct. 4, 2024.
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/lads/" itemprop="url">[ICLR&#39;25] Discrete Latent Plans via Semantic Skill Abstractions</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Haobin Jiang, Jiangxing Wang, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    The Thirteenth International Conference on Learning Representations (ICLR), April 24-28, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 32.08%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/jept/" itemprop="url">[ICLR&#39;25] Learning Video-Conditioned Policy on Unlabelled Data with Joint Embedding Predictive Transformer</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Hao Luo and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    The Thirteenth International Conference on Learning Representations (ICLR), April 24-28, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 32.08%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/mart/" itemprop="url">[ICLR&#39;25] MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Junpeng Yue, Xinrun Xu, Börje F. Karlsson, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    The Thirteenth International Conference on Learning Representations (ICLR), April 24-28, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 32.08%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/wldm/" itemprop="url">[ICLR&#39;25] Watch Less, Do More: Implicit Skill Discovery for Video-Conditioned Policy</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Jiangxing Wang and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    The Thirteenth International Conference on Learning Representations (ICLR), April 24-28, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 32.08%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/crossdex/" itemprop="url">[ICLR&#39;25] Cross-Embodiment Dexterous Grasping with Reinforcement Learning</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Haoqi Yuan, Bohan Zhou, Yuhui Fu, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    The Thirteenth International Conference on Learning Representations (ICLR), April 24-28, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 32.08%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/resdex/" itemprop="url">[ICLR&#39;25] Efficient Residual Learning with Mixture-of-Experts for Universal Dexterous Grasping</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Ziye Huang, Haoqi Yuan, Yuhui Fu, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    The Thirteenth International Conference on Learning Representations (ICLR), April 24-28, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 32.08%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/bpe/" itemprop="url">[ICLR&#39;25] From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Wanpeng Zhang, Zilong Xie, Yicheng Feng, Yijiang Li, Xingrun Xing, Sipeng Zheng, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    The Thirteenth International Conference on Learning Representations (ICLR), April 24-28, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 32.08%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/puppet/" itemprop="url">[ICML&#39;25] Scaling Large Motion Models with Million-Level Human Motions</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Ye Wang, Sipeng Zheng, Bin Cao, Qianshan Wei, Weishuai Zeng, Qin Jin, Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    Forty-Second International Conference on Machine Learning (ICML), July 13-19, 2025
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 26.9%=<sup>3260</sup>&frasl;<sub>12107</sub>)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/nolo/" itemprop="url">[IROS&#39;25] NOLO: Navigate Only Look Once</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Bohan Zhou, Zhongbin Zhang, Jiangxing Wang, Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), October 19-25, 2025.
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/motionctrl/" itemprop="url">[ICCV&#39;25] MotionCtrl: A Real-Time Controllable Vision-Language-Motion Model</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Bin Cao, Sipeng Zheng, Ye Wang, Lujie Xia, Qianshan Wei, Qin Jin, Jing Liu, Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    International Conference on Computer Vision (ICCV), October 19-23, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 24%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/unibpe/" itemprop="url">[ICCV&#39;25] Unified Multimodal Understanding via Byte-Pair Visual Encoding</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Wanpeng Zhang, Yicheng Feng, Hao Luo, Yijiang Li, Zihao Yue, Sipeng Zheng, Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    International Conference on Computer Vision (ICCV), October 19-23, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 24%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/selu/" itemprop="url">[TMLR] SELU: Self-Learning Embodied Multimodal Large Language Models in Unknown Environments</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Boyu Li, Haobin Jiang, Ziluo Ding, Xinrun Xu, Haoran Li, Dongbin Zhao, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    Transactions on Machine Learning Research (TMLR), 2025
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/bumblebee/" itemprop="url">[NeurIPS&#39;25] From Experts to a Generalist: Toward General Whole-Body Control for Humanoid Robots</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Yuxuan Wang, Ming Yang, Weishuai Zeng, Yu Zhang, Xinrun Xu, Haobin Jiang, Ziluo Ding, Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    Thirty-Ninth Conference on Neural Information Processing Systems (NeurIPS), Dec. 2-7, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 24.52%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/megohand/" itemprop="url">[NeurIPS&#39;25] MEgoHand: Multi-Modal Egocentric Hand-Object Interaction Motion Generation</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Bohan Zhou, Yi Zhan, Zhongbin Zhang, Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    Thirty-Ninth Conference on Neural Information Processing Systems (NeurIPS), Dec. 2-7, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 24.52%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/openmmego/" itemprop="url">[NeurIPS&#39;25] OpenMMEgo: Enhancing Egocentric Understanding for LMMs with Open Weights and Data</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Hao Luo, Zihao Yue, Wanpeng Zhang, Yicheng Feng, Sipeng Zheng, Deheng Ye, Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    Thirty-Ninth Conference on Neural Information Processing Systems (NeurIPS), Dec. 2-7, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 24.52%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
        
        </div>
    </article>
    
    <nav>
    <ul class="pager">
    	
        
        

        
    </ul>
</nav>


</div>
<footer class="site-footer">
    <div class="container">
        <p class="powered-by">

            &copy; 2025 Zongqing Lu &middot; 

            Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

            <span class="pull-right"><a href="#" id="back_to_top"><span class="button_icon"><i class="fa fa-chevron-up fa-2x" aria-hidden="true"></i></span></a></span>

        </p>
    </div>
</footer>

        <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
        <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
        <script src="https://z0ngqing.github.io/js/jquery-1.12.3.min.js"></script>
        <script src="https://z0ngqing.github.io/js/bootstrap.min.js"></script>
        <script src="https://z0ngqing.github.io/js/hugo-academic.js"></script>
        

        
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
            ga('create', 'G-ZKGTRY33CM', 'auto');
            ga('send', 'pageview');

             
            var links = document.querySelectorAll('a');
            Array.prototype.map.call(links, function(item) {
                if (item.host != document.location.host) {
                    item.addEventListener('click', function() {
                        var action = item.getAttribute('data-action') || 'follow';
                        ga('send', 'event', 'outbound', action, item.href);
                    });
                }
            });
        </script>
        

        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>

        
        
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>
        

    </body>
</html>

