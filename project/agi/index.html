<!DOCTYPE html>
<html lang="en-us">
<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="theme" content="hugo-academic">
    <meta name="generator" content="Hugo 0.55.0" />
    <meta name="author" content="Zongqing Lu">
    <meta name="description" content="Associate Professor">

    <link rel="stylesheet" href="https://z0ngqing.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="https://z0ngqing.github.io/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://z0ngqing.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://z0ngqing.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700|Merriweather|Roboto+Mono">
    <link rel="stylesheet" href="https://z0ngqing.github.io/css/hugo-academic.css">
    


    <link rel="shortcut icon" href="https://z0ngqing.github.io/img/favicon.ico" type="image/x-icon">
    <link rel="canonical" href="https://z0ngqing.github.io/project/agi/">

    <title>Generalist Agents | Zongqing&#39;s Homepage</title>

</head>
<body id="top">


<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
    <div class="container">

        
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://z0ngqing.github.io/">Zongqing&#39;s Homepage</a>
        </div>

        
        <div class="collapse navbar-collapse" id="#navbar-collapse-1">

            
            <ul class="nav navbar-nav navbar-right">
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#top">Home</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#news">News</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#publications">Publications</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#projects">Lab</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#teaching">Teaching</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#services">Services</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#contact">Contact</a></li>
                
            </ul>

        </div>
    </div>
</nav>

<div class="container">

    <article class="article article-project" itemscope itemtype="http://schema.org/Article">
        

        <h1 itemprop="name">Generalist Agents</h1>
        
        

<div class="article-metadata">

    
    
    
    <span class="article-tags">
        <i class="fa fa-tags"></i>
        
        <a class="article-tag-link" href="https://z0ngqing.github.io/tags/reinforcement-learning">Reinforcement Learning</a>, 
        
        <a class="article-tag-link" href="https://z0ngqing.github.io/tags/foundation-models">Foundation Models</a>
        
    </span>
    
    

    

</div>


        

        <div align="justify" class="article-style" itemprop="articleBody">
            

<p>Recently developed foundation models, such as large language models and multi-modal models, open great opportunities to build generally capable agents, combined with reinforcement learning. This project focuses on learning skills and foundation models and connecting them to build generalist agents. In the following, we introduce some of our studies. For more details, please refer to the papers.</p>

<h3 id="plan4mc">Plan4MC</h3>

<p><img style="float: right;  margin: 10px 0px 0px 20px;" src = "/img/banners/plan4mc.png" width="550" class="article-style" itemprop="image">
  We study building a multi-task agent in Minecraft. Without human demonstrations, solving long-horizon tasks in this open-ended environment with reinforcement learning (RL) is extremely sample inefficient. To tackle the challenge, we decompose solving Minecraft tasks into learning basic skills and planning over the skills. We propose three types of fine-grained basic skills in Minecraft, and use RL with intrinsic rewards to accomplish basic skills with high success rates. For skill planning, we use Large Language Models to find the relationships between skills and build a skill graph in advance. When the agent is solving a task, our skill search algorithm walks on the skill graph and generates the proper skill plans for the agent. In experiments, our method accomplishes 40 diverse Minecraft tasks, where many tasks require sequentially executing for more than 10 skills. Our method outperforms baselines in most tasks by a large margin.</p>

<h3 id="state-to-go-transformer">State-to-Go Transformer</h3>

<p><img style="float: right;  margin: 10px 0px 0px 20px;" src = "/img/project/stg.jpeg" width="550" class="article-style" itemprop="image">
Learning from visual observation (LfVO), aiming at recovering policies from only visual observation data, is promising yet a challenging problem. Existing LfVO approaches either only adopt inefficient online learning schemes or require additional task-specific information like goal states, making them not suited for open-ended tasks. To address these issues, we propose a two-stage framework for learning from visual observation. In the first stage, we introduce and pretrain State-to-Go (STG) Transformer offline to predict and differentiate latent transitions of demonstrations. Subsequently, in the second stage, the STG Transformer provides intrinsic rewards for downstream reinforcement learning tasks where an agent learns merely from intrinsic rewards. Empirical results on Atari and Minecraft show that our proposed method outperforms baselines and in some tasks even achieves performance comparable to the policy learned from environmental rewards. These results shed light on the potential of utilizing video-only data to solve difficult visual reinforcement learning tasks rather than relying on complete offline datasets containing states, actions, and rewards.</p>

        </div>


        <div align="justify" class="article-style" itemprop="articleBody">
            
                
            
                
            
                
            
                
            
                
            
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
            
                
            
        </div>

        <div>
        
            <br>
            <h3>Publications</h3>
            
                
                    
                
            
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/stg/" itemprop="url">[NeurIPS&#39;23] Learning from Visual Observation via Offline Pretrained State-to-Go Transformer</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Bohan Zhou, Ke Li, Jiechuan Jiang, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    Thirty-Seventh Conference on Neural Information Processing Systems (NeurIPS), Dec. 10-16, 2023.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 26.1%=<sup>3221</sup>&frasl;<sub>12343</sub>)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/plan4mc/" itemprop="url">Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    NeurIPS 2023 Workshop on Foundation Models for Decision Making.
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/ptgm/" itemprop="url">[ICLR&#39;24] Pre-Training Goal-Based Models for Sample-Efficient Reinforcement Learning</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Haoqi Yuan, Zhancun Mu, Feiyang Xie, Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    <em>Eighth International Conference on Learning Representations (ICLR)</em>, May 7-11, 2024.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 1.2%, <em>oral</em>)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/steve-eye/" itemprop="url">[ICLR&#39;24] Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Sipeng Zheng, Jiazheng Liu, Yicheng Feng, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    Eighth International Conference on Learning Representations (ICLR), May 7-11, 2024.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 31%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/adarefiner/" itemprop="url">[NAACL&#39;24] AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Wanpeng Zhang and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), <em>Findings</em>, June 16–21, 2024.
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/llama-rider/" itemprop="url">[NAACL&#39;24] LLaMA Rider: Spurring Large Language Models to Explore the Open World</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Yicheng Feng, Yuxuan Wang, Jiazheng Liu, Sipeng Zheng, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), <em>Findings</em>, June 16–21, 2024.
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/cradle/" itemprop="url">Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, Yifei Bi, Pengjie Gu, Xinrun Wang, Börje F Karlsson, Bo An, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    ICLR 2024 Workshop on LLM Agents.
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/pvdr/" itemprop="url">[ECCV&#39;24] Pre-Trained Visual Dynamics Representations for Efficient Policy Learning</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Hao Luo, Bohan Zhou, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    European Conference on Computer Vision (ECCV), Sep. 29- Oct. 4, 2024.
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/clip4mc/" itemprop="url">[ECCV&#39;24] Reinforcement Learning Friendly Vision-Language Model for Minecraft</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Haobin Jiang, Junpeng Yue, Hao Luo, Ziluo Ding, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    European Conference on Computer Vision (ECCV), Sep. 29- Oct. 4, 2024.
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/unicode/" itemprop="url">[ECCV&#39;24] UniCode: Learning a Unified Codebook for Multimodal Large Language Models</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Sipeng Zheng, Bohan Zhou, Yicheng Feng, Ye Wang, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    European Conference on Computer Vision (ECCV), Sep. 29- Oct. 4, 2024.
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/copl/" itemprop="url">[ECCV&#39;24] Visual Grounding for Object-Level Generalization in Reinforcement Learning</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Haobin Jiang and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    European Conference on Computer Vision (ECCV), Sep. 29- Oct. 4, 2024.
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/rl-gpt/" itemprop="url">[NeurIPS&#39;24] RL-GPT: Integrating Reinforcement Learning and Code-as-policy</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, and Jiaya Jia
                
            </div>

            <div class="pub-publication">
                
                    Thirty-Eighth Conference on Neural Information Processing Systems (NeurIPS), Dec. 9-15, 2024.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 25.8%, <em>oral</em>)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/cradle-iclr/" itemprop="url">Cradle: Empowering Foundation Agents towards General Computer Control</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, Ruyi An, Molei Qin, Chuqiao Zong, Longtao Zheng, YuJie Wu, Xiaoqiang Chai, Yifei Bi, Tianbao Xie, Pengjie Gu, Xiyun Li, Ceyao Zhang, Long Tian, Chaojie Wang, Xinrun Wang, Börje F. Karlsson, Bo An, Shuicheng YAN, Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    NeurIPS 2024 Workshop on Open-World Agents
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/emo/" itemprop="url">[NAACL&#39;25] LLM-Based Explicit Models of Opponents for Multi-Agent Games</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Xiaopeng Yu, Wanpeng Zhang, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL), April 29- May 4, 2025.
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/jept/" itemprop="url">[ICLR&#39;25] Learning Video-Conditioned Policy on Unlabelled Data with Joint Embedding Predictive Transformer</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Hao Luo and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    The Thirteenth International Conference on Learning Representations (ICLR), April 24-28, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 32.08%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/mart/" itemprop="url">[ICLR&#39;25] MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Junpeng Yue, Xinrun Xu, Börje F. Karlsson, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    The Thirteenth International Conference on Learning Representations (ICLR), April 24-28, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 32.08%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/bpe/" itemprop="url">[ICLR&#39;25] From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Wanpeng Zhang, Zilong Xie, Yicheng Feng, Yijiang Li, Xingrun Xing, Sipeng Zheng, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    The Thirteenth International Conference on Learning Representations (ICLR), April 24-28, 2025.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 32.08%)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
            
                
                    
                
            
        
        </div>
    </article>
    
    <nav>
    <ul class="pager">
    	
        
        

        
    </ul>
</nav>


</div>
<footer class="site-footer">
    <div class="container">
        <p class="powered-by">

            &copy; 2024 Zongqing Lu &middot; 

            Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

            <span class="pull-right"><a href="#" id="back_to_top"><span class="button_icon"><i class="fa fa-chevron-up fa-2x" aria-hidden="true"></i></span></a></span>

        </p>
    </div>
</footer>

        <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
        <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
        <script src="https://z0ngqing.github.io/js/jquery-1.12.3.min.js"></script>
        <script src="https://z0ngqing.github.io/js/bootstrap.min.js"></script>
        <script src="https://z0ngqing.github.io/js/hugo-academic.js"></script>
        

        
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
            ga('create', 'UA-88925956-1', 'auto');
            ga('send', 'pageview');

             
            var links = document.querySelectorAll('a');
            Array.prototype.map.call(links, function(item) {
                if (item.host != document.location.host) {
                    item.addEventListener('click', function() {
                        var action = item.getAttribute('data-action') || 'follow';
                        ga('send', 'event', 'outbound', action, item.href);
                    });
                }
            });
        </script>
        

        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>

        
        
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>
        

    </body>
</html>

