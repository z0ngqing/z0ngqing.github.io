<!DOCTYPE html>
<html lang="en-us">
<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="theme" content="hugo-academic">
    <meta name="generator" content="Hugo 0.55.0" />
    <meta name="author" content="Zongqing Lu">
    <meta name="description" content="BOYA Assistant Professor">

    <link rel="stylesheet" href="https://z0ngqing.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="https://z0ngqing.github.io/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://z0ngqing.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://z0ngqing.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700|Merriweather|Roboto+Mono">
    <link rel="stylesheet" href="https://z0ngqing.github.io/css/hugo-academic.css">
    


    <link rel="shortcut icon" href="https://z0ngqing.github.io/img/favicon.ico" type="image/x-icon">
    <link rel="canonical" href="https://z0ngqing.github.io/project/agi/">

    <title>Generalist Agents | Zongqing&#39;s Homepage</title>

</head>
<body id="top">


<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
    <div class="container">

        
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://z0ngqing.github.io/">Zongqing&#39;s Homepage</a>
        </div>

        
        <div class="collapse navbar-collapse" id="#navbar-collapse-1">

            
            <ul class="nav navbar-nav navbar-right">
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#top">Home</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#news">News</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#publications">Publications</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#projects">Lab</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#teaching">Teaching</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#services">Services</a></li>
                
                <li class="nav-item"><a href="https://z0ngqing.github.io/#contact">Contact</a></li>
                
            </ul>

        </div>
    </div>
</nav>

<div class="container">

    <article class="article article-project" itemscope itemtype="http://schema.org/Article">
        

        <h1 itemprop="name">Generalist Agents</h1>
        
        

<div class="article-metadata">

    
    
    
    <span class="article-tags">
        <i class="fa fa-tags"></i>
        
        <a class="article-tag-link" href="https://z0ngqing.github.io/tags/reinforcement-learning">Reinforcement Learning</a>, 
        
        <a class="article-tag-link" href="https://z0ngqing.github.io/tags/foundation-models">Foundation Models</a>
        
    </span>
    
    

    

</div>


        

        <div align="justify" class="article-style" itemprop="articleBody">
            

<p>Recently developed foundation models, such as large language models and multi-modal models, open great opportunities to build generally capable agents, combined with reinforcement learning. This project focuses on learning skills and foundation models and connecting them to build generalist agents. In the following, we introduce some of our studies. For more details, please refer to the papers.</p>

<h3 id="plan4mc">Plan4MC</h3>

<p><img style="float: right;  margin: 10px 0px 0px 20px;" src = "/img/banners/plan4mc.png" width="550" class="article-style" itemprop="image">
  We study building a multi-task agent in Minecraft. Without human demonstrations, solving long-horizon tasks in this open-ended environment with reinforcement learning (RL) is extremely sample inefficient. To tackle the challenge, we decompose solving Minecraft tasks into learning basic skills and planning over the skills. We propose three types of fine-grained basic skills in Minecraft, and use RL with intrinsic rewards to accomplish basic skills with high success rates. For skill planning, we use Large Language Models to find the relationships between skills and build a skill graph in advance. When the agent is solving a task, our skill search algorithm walks on the skill graph and generates the proper skill plans for the agent. In experiments, our method accomplishes 40 diverse Minecraft tasks, where many tasks require sequentially executing for more than 10 skills. Our method outperforms baselines in most tasks by a large margin.</p>

<h3 id="state-to-go-transformer">State-to-Go Transformer</h3>

<p><img style="float: right;  margin: 10px 0px 0px 20px;" src = "/img/project/stg.jpeg" width="550" class="article-style" itemprop="image">
Learning from visual observation (LfVO), aiming at recovering policies from only visual observation data, is promising yet a challenging problem. Existing LfVO approaches either only adopt inefficient online learning schemes or require additional task-specific information like goal states, making them not suited for open-ended tasks. To address these issues, we propose a two-stage framework for learning from visual observation. In the first stage, we introduce and pretrain State-to-Go (STG) Transformer offline to predict and differentiate latent transitions of demonstrations. Subsequently, in the second stage, the STG Transformer provides intrinsic rewards for downstream reinforcement learning tasks where an agent learns merely from intrinsic rewards. Empirical results on Atari and Minecraft show that our proposed method outperforms baselines and in some tasks even achieves performance comparable to the policy learned from environmental rewards. These results shed light on the potential of utilizing video-only data to solve difficult visual reinforcement learning tasks rather than relying on complete offline datasets containing states, actions, and rewards.</p>

        </div>


        <div align="justify" class="article-style" itemprop="articleBody">
            
                
            
                
            
                
            
                
            
                
            
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
            
                
            
        </div>

        <div>
        
            <br>
            <h3>Publications</h3>
            
                
                    
                
            
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/plan4mc/" itemprop="url">Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    arXiv:2303.16563
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/rladapter/" itemprop="url">RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Wanpeng Zhang and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    arXiv:2309.17176
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/stg/" itemprop="url">[NeurIPS&#39;23] Learning from Visual Observation via Offline Pretrained State-to-Go Transformer</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Bohan Zhou, Ke Li, Jiechuan Jiang, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    Thirty-Seventh Conference on Neural Information Processing Systems (NeurIPS), Dec. 10-16, 2023.
                
            </div>
            <div class="pub-publication">
                
                    (Acceptance Rate: 26.1%=<sup>3221</sup>&frasl;<sub>12343</sub>)
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/llama-rider/" itemprop="url">LLaMA Rider: Spurring Large Language Models to Explore the Open World</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Yicheng Feng, Yuxuan Wang, Jiazheng Liu, Sipeng Zheng, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    arXiv:2310.08922
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
                
            
                
                    
                        <dl> <div itemscope itemtype="http://schema.org/CreativeWork">
    <div class="row">
        <div class="col-md-12">

            <span itemprop="name">
                <a href="https://z0ngqing.github.io/publication/steve-eye/" itemprop="url">Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds</a>
            </span>

            <div class="pub-authors" itemprop="author">
                
                Sipeng Zheng, Jiazheng Liu, Yicheng Feng, and Zongqing Lu
                
            </div>

            <div class="pub-publication">
                
                    arXiv:2310.13255
                
            </div>
            <div class="pub-publication">
                
            </div>


        </div>
    </div>
</div>
 </dl>
                    
                
            
        
        </div>
    </article>
    
    <nav>
    <ul class="pager">
    	
        
        

        
    </ul>
</nav>


</div>
<footer class="site-footer">
    <div class="container">
        <p class="powered-by">

            &copy; 2022 Zongqing Lu &middot; 

            Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

            <span class="pull-right"><a href="#" id="back_to_top"><span class="button_icon"><i class="fa fa-chevron-up fa-2x" aria-hidden="true"></i></span></a></span>

        </p>
    </div>
</footer>

        <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
        <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
        <script src="https://z0ngqing.github.io/js/jquery-1.12.3.min.js"></script>
        <script src="https://z0ngqing.github.io/js/bootstrap.min.js"></script>
        <script src="https://z0ngqing.github.io/js/hugo-academic.js"></script>
        

        
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
            ga('create', 'UA-88925956-1', 'auto');
            ga('send', 'pageview');

             
            var links = document.querySelectorAll('a');
            Array.prototype.map.call(links, function(item) {
                if (item.host != document.location.host) {
                    item.addEventListener('click', function() {
                        var action = item.getAttribute('data-action') || 'follow';
                        ga('send', 'event', 'outbound', action, item.href);
                    });
                }
            });
        </script>
        

        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>

        
        
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>
        

    </body>
</html>

